---
layout: post
title: 多任务深度学习
category: DL
comments: false
---
##### 简介
多任务深度网络的结构：Input xi表示不同任务的输入数据，**一般** 位于低层次的网络表示不同任务之间共享的层，不同分支的高层次网络表示每个任务特定的层，Task xi表示不同任务对应的损失函数层。

在多任务深度网络中，低层次语义信息的共享有助于减少计算量，同时共享表示层可以使得几个有共性的任务更好的结合相关性信息，任务特定层则可以单独建模任务特定的信息，实现共享信息和任务特定信息的统一。

此外，在深度网络中，多任务的语义信息还可以从不同的层输出，例如GoogLeNet中的两个辅助损失层。另外一个例子比如衣服图像检索系统，颜色这类的信息可以从较浅层的时候就进行输出判断，而衣服的样式风格这类的信息，更接近高层语义，需要从更高的层次进行输出，这里的输出指的是每个任务对应的损失层的前一层。

##### 应用
1.faster-rcnn，包含2个任务，候选窗口的生成(分类)和窗口回归  
2.面部关键点估计和头部姿态以及人脸属性（是否戴眼镜、是否微笑和性别）之间有着紧密的联系，可以同时进行关键点检测以及人脸属性的分类  
**3**.图像检索系统，比如衣服搜索，因为不同的衣服有不同的属性，如颜色，样式..属于细粒度分类问题，同时，要想检索到类似的衣服，需要进行特征抽取以及hashing,所以往往将细粒度分类和hashing结合到一起，提高检索的性能

##### 实现
以caffe为例：  
1.输入层：  
多任务，往往对应着数据的多标签，所以需要修改输入层，实现多标签输入，或者，数据和标签分别作为数据层输入，如果根据不同任务需要对标签层切分的话，可以后面接上slice层。  
2.损失函数层：  
首先，说一下 **loss_weight** :  
*For nets with multiple layers producing a loss (e.g., a network that both classifies the input using a SoftmaxWithLoss layer and reconstructs it using a EuclideanLoss layer), loss weights can be used to specify their relative importance*
也就是说，其实caffe的所有层，都可以产生loss并且后向传播的，而起决定作用的就是 loss_weight，loss_weight为0，不产生loss,非0,产生loss.

现在回到多任务深度学习的损失层上，不同的任务显然有不同的损失函数，所以这个时候 loss_weight就起到了 **损失权重** 的作用。

3.说了头尾（数据和损失层），由于多个任务，往往会涉及到层的切分和叠加，如slice层和concat层等。
