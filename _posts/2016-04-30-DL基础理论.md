---
layout: post
title: DL基础理论
category: DL
comments: true
---
#### 基础结构单元
##### 卷积层  



卷积是深度网络的重要结构单元之一。我们给出了2D卷积的连续和离散形式，注意卷积核需要做中心翻转。在Caffe实现中，卷积核假设已经翻转，所以卷积运算可以看作当前窗口和卷积核的一个 **内积** （不考虑bias项），下图我们给出了一个卷积运算的示意。
![..](https://raw.githubusercontent.com/glbing/blogs/gh-pages/images/123.png)

与全连接层相比，卷积层的输出神经元只和部分输入层神经元连接，同时相同响应图内，不同空间位置共享卷积核参数，因此卷积层大大降低了要学习的参数数量。

Caffe当中，为了避免卷积运算中频繁的内存访问和过深的循序嵌套，对卷积运算进行了以下加速：通过 **Im2Co** 操 作一次取出所有的patch并组成矩阵，与kernel矩阵做乘法运算直接得到卷积结果。
![con2](https://raw.githubusercontent.com/glbing/blogs/gh-pages/images/con2.png)



##### 反卷积层



有卷积就有反卷积，反卷积是卷积的逆运算，实现了信号的复原。在 **全卷积网络** 中，反卷积层实现了图像的上采样，从而得到如输入图像大小相同的输出。
![fcn](https://raw.githubusercontent.com/glbing/blogs/gh-pages/images/fcn.png)

这里 **全卷积网络 FCN** ，值得深入了解,尤其是在图像语义分割方面：end-to-end 的
semantic segmentation ，有较好的应用，[论文](http://www.cs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)  [其他](http://blog.csdn.net/u010678153/article/details/48676195)


##### pooling层

Pooling层一般配合卷积层使用，可以获得特征的不变性。常见的Pooling操作有max pooling、mean pooling和随机pooling。其中max pooling取最大值，mean pooling取均值，随机pooling按响应 值的大小依概率选择，如下。
![pooling](https://raw.githubusercontent.com/glbing/blogs/gh-pages/images/pooling.png)  


##### 激活函数

激活函数一般用于卷积层和全连接层之后，激活函数是深度网络非线性的主要来源。常见的激活函数Sigmoid, 双曲正切，ReLU（生物启发，克服了梯度消失问题）, PReLU（alpha可学习）, ELU和maxout。 其中PReLU和ELU都是ReLU的改进。  
![jihuo](https://raw.githubusercontent.com/glbing/blogs/gh-pages/images/jihuo.png)


##### Dropout

Dropout由Hinton组提出于2012年，Dropout随机将比例为p的神经元输出设置为0，是一种避免深度网络过拟合的随机正则化策略，同时Dropout也可以看作是一种隐式的模型集成。  
![dropout](https://raw.githubusercontent.com/glbing/blogs/gh-pages/images/dropout.png)

未完待续.....
